{
  "gpt-5.1": {
    "provider": "openai",
    "context_length": 32000,
    "supports_images": true,
    "parameters": {
      "reasoning_effort": {
        "type": "enum",
        "options": ["none", "low", "medium", "high"],
        "default": "medium",
        "label": "Reasoning Effort",
        "description": "Controls how much thinking time the model spends."
      },
      "verbosity": {
        "type": "enum",
        "options": ["low", "medium", "high"],
        "default": "high",
        "label": "Verbosity",
        "description": "Controls how long or detailed responses should be.",
        "client_support": ["responses"]
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 8192,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on how many tokens the reply can use."
      }
    }
  },
  "gpt-5": {
    "provider": "openai",
    "context_length": 32000,
    "supports_images": true,
    "parameters": {
      "reasoning_effort": {
        "type": "enum",
        "options": ["minimal", "low", "medium", "high"],
        "default": "medium",
        "label": "Reasoning Effort",
        "description": "Controls how much thinking time the model spends."
      },
      "verbosity": {
        "type": "enum",
        "options": ["low", "medium", "high"],
        "default": "high",
        "label": "Verbosity",
        "description": "Controls how long or detailed responses should be.",
        "client_support": ["responses"]
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 8192,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on how many tokens the reply can use."
      }
    }
  },
  "gpt-4.1": {
    "provider": "openai",
    "context_length": 32000,
    "supports_images": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "0 is deterministic, higher values add randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1.0,
        "label": "Top P",
        "description": "Nucleus sampling threshold."
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 4096,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on reply length."
      }
    }
  },
  "gpt-4o": {
    "provider": "openai",
    "context_length": 32000,
    "supports_images": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1.0,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 4096,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on reply length."
      }
    }
  },
  "gpt-4o-2024-11-20": {
    "provider": "openai",
    "context_length": 32000,
    "supports_images": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1.0,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 4096,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on reply length."
      }
    }
  },
  "o3": {
    "provider": "openai",
    "context_length": 200000,
    "parameters": {
      "reasoning_effort": {
        "type": "enum",
        "options": ["minimal", "low", "medium", "high"],
        "default": "medium",
        "label": "Reasoning Effort",
        "description": "Controls how much time the reasoning model spends thinking."
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 4096,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "chatgpt-4o-latest": {
    "provider": "openai",
    "context_length": 32000,
    "supports_images": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1.0,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_completion_tokens": {
        "type": "int",
        "min": 32,
        "max": 4096,
        "step": 1,
        "default": 4096,
        "label": "Max completion tokens",
        "description": "Upper bound on reply length."
      }
    }
  },
  "grok-4-1-fast-reasoning": {
    "provider": "openai",
    "context_length": 128000,
    "base_url": "https://api.x.ai/v1",
    "api_key_env": "XAI_API_KEY",
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 2048,
        "step": 1,
        "default": 2048,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "grok-4-0709": {
    "provider": "openai",
    "context_length": 32768,
    "base_url": "https://api.x.ai/v1",
    "api_key_env": "XAI_API_KEY",
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 2048,
        "step": 1,
        "default": 2048,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "stockmark/stockmark-2-100b-instruct": {
    "provider": "openai",
    "context_length": 32768,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 2048,
        "step": 1,
        "default": 2048,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "qwen/qwen3-235b-a22b": {
    "provider": "openai",
    "context_length": 32768,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 8192,
        "step": 1,
        "default": 4096,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "qwen/qwen3-next-80b-a3b-instruct": {
    "provider": "openai",
    "context_length": 128000,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 0.6,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 0.7,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 8192,
        "step": 1,
        "default": 4096,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "moonshotai/kimi-k2-instruct-0905": {
    "provider": "openai",
    "context_length": 250000,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 4096,
        "step": 1,
        "default": 4096,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "mistralai/mistral-large-3-675b-instruct-2512": {
    "provider": "openai",
    "context_length": 128000,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.01,
        "default": 0.2,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 260000,
        "step": 1,
        "default": 8000,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "z-ai/glm-4.6": {
    "provider": "openai",
    "context_length": 32000,
    "base_url": "https://openrouter.ai/api/v1",
    "api_key_env": "OPENROUTER_API_KEY",
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 0.95,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 120000,
        "step": 1,
        "default": 8000,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1": {
    "provider": "openai",
    "context_length": 120000,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 1024,
        "step": 1,
        "default": 1024,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "claude-sonnet-4-5": {
    "provider": "anthropic",
    "context_length": 32000,
    "thinking_type": "enabled",
    "thinking_budget": 4096,
    "supports_images": true
  },
  "claude-opus-4-20250514": {
    "provider": "anthropic",
    "context_length": 32000,
    "thinking_type": "enabled",
    "thinking_budget": 8192,
    "supports_images": true
  },
  "gemini-3-pro-preview": {
    "provider": "gemini",
    "context_length": 128000,
    "supports_images": true
  },
  "gemini-2.5-pro": {
    "provider": "gemini",
    "context_length": 128000,
    "supports_images": true
  },
  "gemini-2.5-flash": {
    "provider": "gemini",
    "context_length": 128000,
    "supports_images": true
  },
  "gemini-2.5-flash-preview-09-2025": {
    "provider": "gemini",
    "context_length": 128000,
    "supports_images": true
  },
  "gemini-2.5-flash-lite": {
    "provider": "gemini",
    "context_length": 128000,
    "supports_images": true
  },
  "gemini-2.5-flash-lite-preview-09-2025": {
    "provider": "gemini",
    "context_length": 128000,
    "supports_images": true
  },
  "gemini-2.0-flash": {
    "provider": "gemini",
    "context_length": 1000000,
    "supports_images": true
  },
  "hf.co/unsloth/Qwen3-32B-GGUF:Q4_K_XL": {
    "provider": "ollama",
    "context_length": 32768
  },
  "qwen3:30b": {
    "provider": "ollama",
    "context_length": 32768
  },
  "llama4:16x17b": {
    "provider": "ollama",
    "context_length": 1000000
  },
  "hf.co/unsloth/gemma-3-27b-it-GGUF:Q6_K": {
    "provider": "ollama",
    "context_length": 128000
  },
  "hf.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF:Q6_K_XL": {
    "provider": "ollama",
    "context_length": 128000
  },
  "hf.co/mmnga/llm-jp-3.1-8x13b-instruct4-gguf:Q4_K_M": {
    "provider": "ollama",
    "context_length": 4096
  },
  "hf.co/mmnga/ABEJA-Qwen2.5-32b-Japanese-v1.0-gguf:Q4_K_M": {
    "provider": "ollama",
    "context_length": 32768
  },
  "gemini-1.5-flash": {
    "provider": "gemini",
    "context_length": 1000000,
    "supports_images": true
  },
  "hf.co/unsloth/gemma-3-1b-it-GGUF:BF16": {
    "provider": "ollama",
    "context_length": 32768
  },
  "gpt-oss:20b": {
    "provider": "ollama",
    "context_length": 32768
  },
  "openai/gpt-oss-120b": {
    "provider": "openai",
    "context_length": 120000,
    "base_url": "https://integrate.api.nvidia.com/v1",
    "api_key_env": "NVIDIA_API_KEY",
    "convert_system_to_user": true,
    "parameters": {
      "temperature": {
        "type": "float",
        "min": 0,
        "max": 2,
        "step": 0.1,
        "default": 1,
        "label": "Temperature",
        "description": "Controls randomness."
      },
      "top_p": {
        "type": "float",
        "min": 0,
        "max": 1,
        "step": 0.05,
        "default": 1,
        "label": "Top P",
        "description": "Nucleus sampling cutoff."
      },
      "max_tokens": {
        "type": "int",
        "min": 32,
        "max": 8000,
        "step": 1,
        "default": 8000,
        "label": "Max tokens",
        "description": "Upper bound on generated tokens."
      }
    }
  },
  "hf.co/mradermacher/shisa-v2-llama3.3-70b-i1-GGUF:Q4_K_M": {
    "provider": "ollama",
    "context_length": 32768
  }
}
